#!/bin/bash
# Complete setup for LLM-Orchestrated Penetration Testing Platform

set -e

SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
PROJECT_ROOT="$( cd "$SCRIPT_DIR/.." && pwd )"

echo "=============================================="
echo "LLM-Orchestrated Penetration Testing Platform"
echo "=============================================="
echo ""
echo "This will set up a complete AI-driven pentest system where:"
echo "1. An LLM controls the entire testing process"
echo "2. Remote agents execute commands on target networks"
echo "3. The orchestrator manages workflow and results"
echo ""

# Check prerequisites
echo "Checking prerequisites..."
command -v docker >/dev/null 2>&1 || { echo "Docker is required but not installed. Aborting." >&2; exit 1; }
command -v docker-compose >/dev/null 2>&1 || command -v docker compose >/dev/null 2>&1 || { echo "Docker Compose is required but not installed. Aborting." >&2; exit 1; }

# 1. Fix catalog structure first
echo ""
echo "Step 1: Setting up catalog structure..."
echo "----------------------------------------"
bash "${SCRIPT_DIR}/fix_catalog_setup.sh"

# 2. Run enhanced features setup
echo ""
echo "Step 2: Installing enhanced features..."
echo "----------------------------------------"
if [ -f "${SCRIPT_DIR}/enable_enhanced_features.sh" ]; then
    bash "${SCRIPT_DIR}/enable_enhanced_features.sh"
else
    echo "Enhanced features script not found, skipping..."
fi

# 3. Create LLM orchestrator integration
echo ""
echo "Step 3: Setting up LLM orchestrator..."
echo "----------------------------------------"

# Create router for LLM controller
cat > "${PROJECT_ROOT}/orchestrator/routers/llm_controller.py" << 'EOF'
"""
Router for LLM Controller Integration
"""
from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks
from pydantic import BaseModel
from typing import Dict, Any, List, Optional
import asyncio
import logging

from ..llm_controller import LLMOrchestrator, TestPhase, AgentCommand
from ..auth import get_current_tenant

router = APIRouter(prefix="/v2/llm", tags=["llm_controller"])
logger = logging.getLogger(__name__)

# Store active orchestrators
active_orchestrators = {}

class LLMTestRequest(BaseModel):
    """Request to start LLM-orchestrated test"""
    engagement_id: str
    targets: List[str]
    scope: Dict[str, Any] = {}
    llm_provider: str = "openai"
    auto_approve: bool = False
    phases: List[str] = ["reconnaissance", "enumeration", "vulnerability_assessment"]

class LLMCommandRequest(BaseModel):
    """Manual command request from LLM"""
    engagement_id: str
    commands: List[AgentCommand]

class LLMStatusResponse(BaseModel):
    """Status of LLM-orchestrated test"""
    engagement_id: str
    status: str
    current_phase: str
    commands_executed: int
    findings_count: int
    runtime: str

@router.post("/start_test")
async def start_llm_test(
    request: LLMTestRequest,
    background_tasks: BackgroundTasks,
    tenant_id: str = Depends(get_current_tenant)
):
    """Start an LLM-orchestrated penetration test"""
    
    if request.engagement_id in active_orchestrators:
        raise HTTPException(status_code=400, detail="Test already running for this engagement")
    
    # Create orchestrator
    orchestrator = LLMOrchestrator(
        orchestrator_url="http://localhost:8080",
        llm_provider=request.llm_provider
    )
    
    # Initialize
    await orchestrator.initialize(
        request.engagement_id,
        request.targets,
        request.scope
    )
    
    active_orchestrators[request.engagement_id] = orchestrator
    
    # Start test in background
    background_tasks.add_task(
        run_test_background,
        orchestrator,
        request.engagement_id
    )
    
    return {
        "status": "started",
        "engagement_id": request.engagement_id,
        "message": "LLM-orchestrated test started"
    }

async def run_test_background(orchestrator: LLMOrchestrator, engagement_id: str):
    """Run test in background"""
    try:
        await orchestrator.run_penetration_test()
    except Exception as e:
        logger.error(f"Test failed for {engagement_id}: {e}")
    finally:
        if engagement_id in active_orchestrators:
            del active_orchestrators[engagement_id]
        await orchestrator.cleanup()

@router.get("/status/{engagement_id}")
async def get_test_status(
    engagement_id: str,
    tenant_id: str = Depends(get_current_tenant)
) -> LLMStatusResponse:
    """Get status of LLM-orchestrated test"""
    
    if engagement_id not in active_orchestrators:
        raise HTTPException(status_code=404, detail="Test not found or completed")
    
    orchestrator = active_orchestrators[engagement_id]
    context = orchestrator.context
    
    if not context:
        return LLMStatusResponse(
            engagement_id=engagement_id,
            status="initializing",
            current_phase="unknown",
            commands_executed=0,
            findings_count=0,
            runtime="0:00:00"
        )
    
    return LLMStatusResponse(
        engagement_id=engagement_id,
        status="running",
        current_phase=context.current_phase.value,
        commands_executed=len(context.command_history),
        findings_count=len(context.identified_vulnerabilities),
        runtime=orchestrator._calculate_test_duration()
    )

@router.post("/execute_commands")
async def execute_llm_commands(
    request: LLMCommandRequest,
    tenant_id: str = Depends(get_current_tenant)
):
    """Execute specific commands from LLM"""
    
    if request.engagement_id not in active_orchestrators:
        raise HTTPException(status_code=404, detail="Test not active")
    
    orchestrator = active_orchestrators[request.engagement_id]
    results = []
    
    for command in request.commands:
        try:
            result = await orchestrator.execute_command_on_agent(command)
            results.append(result)
        except Exception as e:
            results.append({"error": str(e), "command": command.command})
    
    return {"results": results}

@router.post("/stop/{engagement_id}")
async def stop_test(
    engagement_id: str,
    tenant_id: str = Depends(get_current_tenant)
):
    """Stop an LLM-orchestrated test"""
    
    if engagement_id not in active_orchestrators:
        raise HTTPException(status_code=404, detail="Test not found")
    
    orchestrator = active_orchestrators[engagement_id]
    await orchestrator.cleanup()
    del active_orchestrators[engagement_id]
    
    return {"status": "stopped", "engagement_id": engagement_id}

@router.get("/active_tests")
async def list_active_tests(tenant_id: str = Depends(get_current_tenant)):
    """List all active LLM-orchestrated tests"""
    
    return {
        "active_tests": list(active_orchestrators.keys()),
        "count": len(active_orchestrators)
    }
EOF

echo "✓ LLM controller router created"

# 4. Create environment configuration
echo ""
echo "Step 4: Creating configuration files..."
echo "----------------------------------------"

cat > "${PROJECT_ROOT}/.env.llm" << 'EOF'
# LLM-Orchestrated Penetration Testing Configuration

# Core Orchestrator
ORCHESTRATOR_URL=http://localhost:8080
TENANT_ID=t_demo
EVIDENCE_DIR=/evidence

# LLM Providers (configure at least one)
# OpenAI (Recommended for best results)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4o
OPENAI_BASE_URL=https://api.openai.com/v1

# Anthropic Claude
ANTHROPIC_API_KEY=your_anthropic_api_key_here
ANTHROPIC_MODEL=claude-3-opus-20240229

# Local LLM (Ollama, LM Studio, etc.)
LOCAL_LLM_URL=http://localhost:11434
LOCAL_LLM_MODEL=mixtral

# Agent Configuration
ALLOW_ACTIVE_SCAN=0  # Set to 1 for active scanning
ALLOW_EXPLOITATION=0  # Set to 1 for exploitation (DANGEROUS!)
SAFE_MODE=1          # Always keep at 1 unless authorized

# Remote Agent Configuration
KALI_SSH_HOST=
KALI_SSH_USER=kali
KALI_SSH_KEY_PATH=~/.ssh/id_rsa

# Database
DATABASE_URL=postgresql://pentest:pentest@db:5432/pentest

# Security
REQUIRE_APPROVAL=true
MAX_CONCURRENT_JOBS=5
MAX_TEST_DURATION=86400  # 24 hours

# Notifications
SLACK_WEBHOOK_URL=
EMAIL_NOTIFICATIONS=false
EOF

echo "✓ Environment configuration created"

# 5. Create quick start script
echo ""
echo "Step 5: Creating quick start script..."
echo "---------------------------------------"

cat > "${PROJECT_ROOT}/start_llm_pentest.sh" << 'EOF'
#!/bin/bash
# Quick start for LLM-Orchestrated Penetration Testing

set -e

echo "Starting LLM-Orchestrated Penetration Testing Platform..."

# Load environment
if [ -f .env.llm ]; then
    export $(cat .env.llm | grep -v '^#' | xargs)
fi

# Check for LLM API key
if [ -z "$OPENAI_API_KEY" ] && [ -z "$ANTHROPIC_API_KEY" ] && [ -z "$LOCAL_LLM_URL" ]; then
    echo "⚠️  WARNING: No LLM provider configured!"
    echo "Please set OPENAI_API_KEY, ANTHROPIC_API_KEY, or LOCAL_LLM_URL in .env.llm"
    exit 1
fi

# Start core services
echo "Starting core services..."
docker compose -f infra/docker-compose.v2.yml up -d

# Wait for services
echo "Waiting for services to be ready..."
sleep 10

# Run migrations
echo "Running database migrations..."
docker compose -f infra/docker-compose.v2.yml exec orchestrator alembic upgrade head

# Create default agent token
echo "Creating agent tokens..."
TOKEN=$(curl -s -X POST http://localhost:8080/v2/agent_tokens \
    -H 'Content-Type: application/json' \
    -H 'X-Dev-User: admin' \
    -H 'X-Dev-Email: admin@example.com' \
    -H 'X-Tenant-Id: t_demo' \
    -d '{"tenant_id":"t_demo","name":"llm-agent"}' | jq -r .token)

echo "Agent token created: ${TOKEN:0:20}..."

# Start a basic agent if none running
if [ -n "$KALI_SSH_HOST" ]; then
    echo "Starting Kali remote agent..."
    AGENT_TOKEN=$TOKEN bash scripts/agent_kali_remote_up.sh &
else
    echo "Starting local dev agent..."
    AGENT_TOKEN=$TOKEN bash scripts/agent_devext_up.sh &
fi

echo ""
echo "=============================================="
echo "Platform is ready!"
echo "=============================================="
echo ""
echo "Orchestrator UI: http://localhost:8080/ui"
echo "API Docs: http://localhost:8080/docs"
echo ""
echo "To start an LLM-controlled pentest:"
echo ""
echo "python orchestrator/llm_controller.py \\"
echo "    --engagement-id test-001 \\"
echo "    --targets 192.168.1.0/24 \\"
echo "    --llm-provider openai"
echo ""
echo "Or use the API:"
echo ""
echo "curl -X POST http://localhost:8080/v2/llm/start_test \\"
echo "    -H 'Content-Type: application/json' \\"
echo "    -H 'X-Dev-User: admin' \\"
echo "    -H 'X-Dev-Email: admin@example.com' \\"
echo "    -H 'X-Tenant-Id: t_demo' \\"
echo "    -d '{"
echo '        "engagement_id": "test-001",'
echo '        "targets": ["192.168.1.0/24"],'
echo '        "llm_provider": "openai"'
echo "    }'"
echo ""
echo "=============================================="
EOF

chmod +x "${PROJECT_ROOT}/start_llm_pentest.sh"
echo "✓ Quick start script created"

# 6. Create test script
echo ""
echo "Step 6: Creating test script..."
echo "--------------------------------"

cat > "${PROJECT_ROOT}/test_llm_pentest.py" << 'EOF'
#!/usr/bin/env python3
"""
Test script for LLM-Orchestrated Penetration Testing
"""
import asyncio
import sys
import os
sys.path.append('orchestrator')

from llm_controller import LLMOrchestrator

async def test_basic_scan():
    """Test basic LLM-controlled scan"""
    
    orchestrator = LLMOrchestrator(
        orchestrator_url="http://localhost:8080",
        llm_provider="openai"  # or "anthropic" or "local"
    )
    
    # Initialize test
    await orchestrator.initialize(
        engagement_id="test-001",
        targets=["scanme.nmap.org"],  # Safe test target
        scope={
            "type": "external",
            "allowed_phases": ["reconnaissance", "enumeration"],
            "max_duration": 3600
        }
    )
    
    print("Starting LLM-controlled penetration test...")
    print(f"Active agents: {orchestrator.active_agents}")
    
    # Run test
    report = await orchestrator.run_penetration_test()
    
    print("\nTest Complete!")
    print(f"Services discovered: {len(report['technical_findings']['services_discovered'])}")
    print(f"Vulnerabilities found: {len(report['technical_findings']['vulnerabilities'])}")
    print(f"Risk assessment: {report['risk_assessment']['overall_risk']}")
    
    await orchestrator.cleanup()

if __name__ == "__main__":
    asyncio.run(test_basic_scan())
EOF

chmod +x "${PROJECT_ROOT}/test_llm_pentest.py"
echo "✓ Test script created"

# 7. Final summary
echo ""
echo "=============================================="
echo "Setup Complete!"
echo "=============================================="
echo ""
echo "Your LLM-Orchestrated Penetration Testing Platform is ready!"
echo ""
echo "Architecture:"
echo "  • LLM Brain: Controls entire testing process"
echo "  • Orchestrator: Manages agents and workflow"
echo "  • Remote Agents: Execute commands on target networks"
echo "  • Automated Workflow: Recon → Enum → Vuln → Exploit → Report"
echo ""
echo "Next steps:"
echo ""
echo "1. Configure your LLM provider in .env.llm:"
echo "   - Set OPENAI_API_KEY for GPT-4"
echo "   - Or ANTHROPIC_API_KEY for Claude"
echo "   - Or LOCAL_LLM_URL for local models"
echo ""
echo "2. Start the platform:"
echo "   ./start_llm_pentest.sh"
echo ""
echo "3. Deploy agents on target networks:"
echo "   - Kali Linux: Use scripts/install_kali_os_agent.sh"
echo "   - Docker: Use the agent containers"
echo "   - Windows/Linux: Deploy raw agents"
echo ""
echo "4. Run an LLM-controlled test:"
echo "   python test_llm_pentest.py"
echo ""
echo "=============================================="
echo "⚠️  SECURITY REMINDERS:"
echo "=============================================="
echo "• ALWAYS get written authorization before testing"
echo "• Keep ALLOW_EXPLOITATION=0 unless explicitly authorized"
echo "• Use read-only credentials for cloud scanning"
echo "• Monitor LLM decisions - it controls real tools!"
echo "• Test in isolated environments first"
echo "=============================================="